/**
 * ==========================================================
 * üåê PixelBeav Proxy Server ‚Äì server.cjs (ENDG√úLTIG)
 * Version: 1.9.0.C (Feature: Automatische Backup-Bereinigung)
 * ==========================================================
 * Enth√§lt folgende Routen/Funktionen:
 * * üõ†Ô∏è CORE ROUTEN
 * ‚úî /health                       ‚Äì Systemstatus
 * ‚úî /debug/head-test              ‚Äì Header & Token-Test
 * * üìÇ GITHUB CRUD-ROUTEN (Mit API-Key Schutz)
 * ‚úî /contents/                    ‚Äì Root-Listing (GET)
 * ‚úî /contents/:path(*)            ‚Äì Datei/Ordner abrufen (GET)
 * ‚úî /contents/:path(*) (PUT)      ‚Äì Datei erstellen/aktualisieren (SHA optional, **Base64-Handling**)
 * ‚úî /contents/:path(*) (DELETE)   ‚Äì Datei l√∂schen (Ben√∂tigt SHA)
 * ‚úî /contents/:path(*)/delete     ‚Äì Alternative L√∂schroute (POST, findet SHA)
 * * üîí SICHERHEIT & ABSICHERUNG
 * ‚úî Aggressive Key-Normalisierung gegen Leerzeichen & Doppel-Header
 * ‚úî Bedingtes Auto-Backup (nur bei Datei√§nderung, mit Hash-Pr√ºfung)
 * ‚úî Korrekter CET/CEST-Zeitstempel f√ºr Backups
 * ‚úî NEU: Automatische Bereinigung alter Backups (max. 3 behalten)
 * ==========================================================
 */

const express 
= require("express");
const jwt = require("jsonwebtoken");
const path = require("path");
const fs = require("fs"); 
const crypto = require("crypto");
const fetch = (...a) => import("node-fetch").then(({ default: f }) => f(...a));

try {
  require("dotenv").config(); 
  console.log("‚úÖ Dotenv geladen.");
} catch (e) {
  console.warn("‚ö†Ô∏è Dotenv konnte nicht geladen werden:", e.message);
}

// ‚öôÔ∏è Umgebungsvariablen werden eingelesen
const {
  APP_ID, INSTALLATION_ID, REPO_OWNER, REPO_NAME, BRANCH, APP_PRIVATE_KEY, API_KEY,
  PROXY_APP_ID, PROXY_INSTALLATION_ID, PROXY_PRIVATE_KEY, PROXY_REPO_OWNER, PROXY_REPO_NAME, PROXY_BRANCH,
} = process.env;
// ==========================================================
// üîë ULTIMATIVE PRIVATE KEY VERARBEITUNG
// ==========================================================
const processKey = (key) => {
    if (!key) return null;
let processedKey = key.trim();
    processedKey = processedKey.replace(/\r\n/g, '\n');
    if (!processedKey.includes('\n') && processedKey.includes('\\n')) {
        processedKey = processedKey.replace(/\\n/g, '\n');
}
    const START_TAG = '-----BEGIN RSA PRIVATE KEY-----';
    const END_TAG = '-----END RSA PRIVATE KEY-----';
const content = processedKey
        .replace(/-----BEGIN ([A-Z0-9]+ )?PRIVATE KEY-----/g, '')
        .replace(/-----END ([A-Z0-9]+ )?PRIVATE KEY-----/g, '')
        .trim();
return `${START_TAG}\n${content}\n${END_TAG}`;
};

const APP_KEY = processKey(APP_PRIVATE_KEY);
const PROXY_KEY = processKey(PROXY_PRIVATE_KEY);
// ==========================================================
// üß≠ CORE PR√úFUNG & LOGIK
// ==========================================================

console.log("üîê Starting PixelBeav Proxy...");
if (!APP_ID || !INSTALLATION_ID || !REPO_OWNER || !REPO_NAME || !APP_KEY || !BRANCH) {
  console.error("‚ùå Fehlende Haupt-ENV-Variablen. Proxy kann nicht starten.");
process.exit(1);
}

let cachedToken = { token: null, expiresAt: 0 };
let cachedBackupToken = { token: null, expiresAt: 0 };
function makeJwt(privateKey, appId) {
  const now = Math.floor(Date.now() / 1000);
const payload = { iat: now - 60, exp: now + 9 * 60, iss: appId };
return jwt.sign(payload, privateKey, { algorithm: "RS256" });
}

async function getInstallationToken() {
  const { token, expiresAt } = cachedToken;
const now = Math.floor(Date.now() / 1000);
  if (token && expiresAt > now + 60) return token;
console.log("üîÑ Requesting new GitHub Installation Token (Haupt-App)...");
  const res = await fetch(`https://api.github.com/app/installations/${INSTALLATION_ID}/access_tokens`, {
    method: "POST",
    headers: { Authorization: `Bearer ${makeJwt(APP_KEY, APP_ID)}`, Accept: "application/vnd.github+json" }
  });
const data = await res.json();
  if (!res.ok) throw new Error(`Token Error: ${res.status} ${JSON.stringify(data)}`);
cachedToken = { token: data.token, expiresAt: Math.floor(new Date(data.expires_at).getTime() / 1000) };
  console.log("‚úÖ Installation Token erfolgreich abgerufen.");
  return data.token;
}

async function ghFetch(path, options = {}) {
  const token = await getInstallationToken();
  const url = `https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/${path}`;
console.log("üåê GitHub API:", options.method || "GET", url);

  const res = await fetch(url, {
    ...options,
    headers: {
      Authorization: `token ${token}`,
      Accept: "application/vnd.github+json",
      "Content-Type": "application/json",
      ...(options.headers || {})
    }
  });
const text = await res.text();
  let data;
  try { data = JSON.parse(text);
} catch { data = { raw: text }; }
  if (!res.ok) { console.error(`‚ùå GitHub Error ${res.status}:`, data);
throw new Error(`GitHubError ${res.status}: ${res.statusText}`); }
  console.log(`‚úÖ GitHub OK (${res.status})`);
  return data;
}

// ==========================================================
// üåê EXPRESS APP & MIDDLEWARE
// ==========================================================
const app = express();
app.use(express.json({ limit: "5mb" }));
app.use((req, _res, next) => {
  console.log(`‚û°Ô∏è  ${req.method} ${req.url} | Body: ${JSON.stringify(req.body || {})}`);
  next();
});
function requireApiKey(req, res, next) {
  const key = req.headers["x-api-key"] || req.query.apiKey;
if (!API_KEY || key !== API_KEY) {
    console.error("üö´ Ung√ºltiger API-Key");
    return res.status(401).json({ error: "unauthorized" });
}
  next();
}

// ==========================================================
// üöÄ REST ROUTEN
// ==========================================================
app.get("/health", (_req, res) => res.json({ status: "ok", repo: REPO_NAME, branch: BRANCH }));
app.get("/debug/head-test", requireApiKey, async (_req, res) => {
  try {
    const data = await ghFetch(`git/refs/heads/${BRANCH}`);
    res.json({ head: data.object.sha });
  } catch (e) { res.status(500).json({ error: e.message }); }
});
app.get("/contents/", requireApiKey, async (_req, res) => {
  try { res.json(await ghFetch("contents")); } catch (e) { res.status(500).json({ error: e.message }); }
});
app.get("/contents/:path(*)", requireApiKey, async (req, res) => {
  const { path: filePath } = req.params;
  try {
    const data = await ghFetch(`contents/${encodeURIComponent(filePath)}`);
    if (Array.isArray(data)) console.log(`üìÅ Folder (${data.length} items)`);
    res.json(data);
  } catch (e) { res.status(500).json({ error: e.message }); }
});
app.put("/contents/:path(*)", requireApiKey, async (req, res) => {
  const { path: filePath } = req.params;
  // NEU: is_base64_encoded Flag hinzugef√ºgt, um die Quelle zu kennzeichnen
  const { message, content, branch, sha, is_base64_encoded } = req.body; 
  if (!message || !content) return res.status(400).json({ error: "message and content required" });

  try {
    let currentSha = sha;
    
    // NEUE LOGIK: SHA automatisch abrufen, wenn er f√ºr ein Update fehlt.
    if (!currentSha) {
      console.log(`‚ÑπÔ∏è SHA fehlt f√ºr ${filePath}. Versuche, Metadaten abzurufen...`);
      try {
        const meta = await ghFetch(`contents/${encodeURIComponent(filePath)}`);
        // Pr√ºfen, ob die Datei existiert und einen SHA hat
        if (meta && meta.sha) {
            currentSha = meta.sha;
            console.log("‚úÖ SHA f√ºr Update automatisch gefunden.");
        }
      } catch (metaError) {
        // Ignoriere 404-Fehler, da dies bedeutet, dass die Datei neu erstellt wird (kein SHA n√∂tig)
        if (metaError.message && !metaError.message.includes("GitHubError 404")) {
           throw metaError; // Andere Fehler weitergeben
        }
        console.log("‚ÑπÔ∏è Datei nicht gefunden (404), wird neu erstellt.");
      }
    }
    
    let contentEncoded;

    // PR√úFUNG: Wenn is_base64_encoded = true, wird der Inhalt direkt verwendet.
    if (is_base64_encoded === true) {
        console.log("‚ÑπÔ∏è Content ist bereits Base64, wird direkt verwendet.");
     
    contentEncoded = content;
    } else {
        // Andernfalls (Standardfall: Rohdaten), wird er neu in Base64 encodiert.
        console.log("‚ÑπÔ∏è Content ist Rohdaten, wird in Base64 encodiert.");
        // Wir encodieren immer von UTF-8 zu Base64
        contentEncoded = Buffer.from(content, 'utf8').toString('base64');
    }
    
    const body = { message, content: contentEncoded, branch: branch ||
BRANCH };
    if (currentSha) body.sha = currentSha;
    
    const data = await ghFetch(`contents/${encodeURIComponent(filePath)}`, {
      method: "PUT", body: JSON.stringify(body)
    });
console.log("‚úÖ File written:", filePath);
    res.json(data);
  } catch (e) { res.status(500).json({ error: e.message }); }
});
app.delete("/contents/:path(*)", (req, res, next) => {
  console.log("üîÅ Redirecting DELETE ‚Üí POST /delete");
  req.url = `/contents/${req.params.path}/delete`;
  req.method = "POST";
  app.handle(req, res, next);
});
app.post("/contents/:path(*)/delete", requireApiKey, async (req, res) => {
  const { path: filePath } = req.params;
  let { message, sha, branch } = req.body;
  branch = branch || BRANCH;

  try {
    if (!sha) {
      const meta = await ghFetch(`contents/${encodeURIComponent(filePath)}`);
      sha = meta.sha;
      console.log("‚úÖ SHA automatisch gefunden.");
    }

    const body = { message: message || `Delete ${filePath}`, sha, branch };
    const data = await ghFetch(`contents/${encodeURIComponent(filePath)}`, {
      method: 
"DELETE", body: JSON.stringify(body)
    });
    console.log("‚úÖ Datei via POST gel√∂scht:", filePath);
    res.json(data);
  } catch (e) { res.status(500).json({ error: e.message }); }
});
// ==========================================================
// üöÄ SERVER START
// ==========================================================
const PORT = process.env.PORT || 8080;
app.listen(PORT, () => console.log(`üöÄ PixelBeav Proxy l√§uft auf Port ${PORT}`));
// ================================================================
// üß© AUTO-BACKUP-SYSTEM (BEDINGT DURCH HASH-PR√úFUNG)
// ================================================================

async function getBackupInstallationToken() {
  const { token, expiresAt } = cachedBackupToken;
const now = Math.floor(Date.now() / 1000);
  if (token && expiresAt > now + 60) return token;
console.log("üîÑ [Proxy-Backup] Requesting new Installation Token...");
  const res = await fetch(`https://api.github.com/app/installations/${PROXY_INSTALLATION_ID}/access_tokens`, {
    method: "POST",
    headers: { Authorization: `Bearer ${makeJwt(PROXY_KEY, PROXY_APP_ID)}`, Accept: "application/vnd.github+json" }
  });
const data = await res.json();
  if (!res.ok) throw new Error(`Backup Token Error: ${res.status} ${JSON.stringify(data)}`);
cachedBackupToken = {
    token: data.token,
    expiresAt: Math.floor(new Date(data.expires_at).getTime() / 1000)
  };
console.log("‚úÖ [Proxy-Backup] Installation Token erfolgreich abgerufen.");
  return data.token;
}

/**
 * Ruft den aktuellen Hash-Wert und den SHA (f√ºr Updates) der Hash-Datei ab.
 * Wenn die Datei nicht existiert (404), wird hash:null und sha:null zur√ºckgegeben.
 * @param {string} token - Der Installations-Token f√ºr das Proxy-Repo.
 * @returns {{hash: string|null, sha: string|null}}
 */
async function getLatestRemoteHash(token) {
    const hashPath = 'backups/current_server_cjs_hash.txt';
const url = `https://api.github.com/repos/${PROXY_REPO_OWNER}/${PROXY_REPO_NAME}/contents/${hashPath}`;

    try {
        // 1. Abrufen der Metadaten (SHA)
        const metaRes = await fetch(url, {
             headers: { Authorization: `token ${token}`, Accept: "application/vnd.github+json" }
        });
if (metaRes.status === 404) {
            console.log("‚ÑπÔ∏è [Proxy-Backup] Keine vorherige Hash-Datei gefunden (404).");
return { hash: null, sha: null };
        }
        if (!metaRes.ok) throw new Error(`Hash Meta Fetch Error: ${metaRes.status}`);
const metaData = await metaRes.json();

        // 2. Abrufen des Inhalts (Hash-Wert)
        const contentRes = await fetch(url, {
            headers: { Authorization: `token ${token}`, Accept: "application/vnd.github.v3.raw" }
        });
if (!contentRes.ok) throw new Error(`Hash Content Fetch Error: ${contentRes.status}`);
        const remoteHash = await contentRes.text();
return { hash: remoteHash.trim(), sha: metaData.sha };
        
    } catch (error) {
        // Bei jedem Fehler im Hash-Prozess (Timeout, etc.) wird der Backup-Prozess neu gestartet
        console.warn("‚ö†Ô∏è [Proxy-Backup] Fehler beim Abrufen/Verarbeiten des Remote-Hash:", error.message);
return { hash: null, sha: null }; 
    }
}

/**
 * L√∂scht automatisch die √§ltesten Backups, sodass nur die MAX_BACKUPS neuesten erhalten bleiben.
 * @param {string} token - Der Installations-Token f√ºr das Proxy-Repo.
 * @param {number} maxBackups - Die maximale Anzahl an Backups, die behalten werden soll.
 */
async function cleanupOldBackups(token, maxBackups) {
    const backupRepoUrl = `https://api.github.com/repos/${PROXY_REPO_OWNER}/${PROXY_REPO_NAME}/contents/backups`;
    const deleteBranch = PROXY_BRANCH || "main";

    try {
        // 1. Liste der Dateien abrufen
        const listRes = await fetch(backupRepoUrl, {
            headers: { Authorization: `token ${token}`, Accept: "application/vnd.github+json" }
        });
        if (!listRes.ok) {
            if (listRes.status === 404) return console.log("‚ÑπÔ∏è [Proxy-Backup] 'backups/' Ordner existiert nicht, keine Bereinigung n√∂tig.");
            throw new Error(`Cleanup List Error: ${listRes.status}`);
        }
        const files = await listRes.json();

        // 2. Filtern und Sortieren der Backup-Dateien
        const backupFiles = files
            // Nur Dateien, die dem Backup-Muster entsprechen
            .filter(f => f.type === 'file' && f.name.startsWith('server_backup_') && f.name.endsWith('.cjs'))
            // Sortieren nach Name (√Ñlteste zuerst, da der Zeitstempel im Namen enthalten ist)
            .sort((a, b) => a.name.localeCompare(b.name));

        if (backupFiles.length <= maxBackups) {
            return console.log(`‚úÖ [Proxy-Backup] Nur ${backupFiles.length} Backups vorhanden (max. ${maxBackups}). Keine Bereinigung n√∂tig.`);
        }

        // Dateien zum L√∂schen: Die √§ltesten (vom Anfang des Arrays), bis nur noch maxBackups √ºbrig sind.
        const filesToDelete = backupFiles.slice(0, backupFiles.length - maxBackups);
        console.log(`üóëÔ∏è [Proxy-Backup] Werde ${filesToDelete.length} √§lteste Backups l√∂schen.`);

        // 3. L√∂schen der √§ltesten Dateien
        for (const file of filesToDelete) {
            const deleteUrl = `${backupRepoUrl}/${file.name}`;
            const deleteBody = JSON.stringify({
                message: `üóëÔ∏è Auto-Cleanup: Delete oldest backup ${file.name}`,
                sha: file.sha, // SHA ist f√ºr das L√∂schen zwingend erforderlich
                branch: deleteBranch
            });

            const deleteRes = await fetch(deleteUrl, {
                method: "DELETE",
                headers: { Authorization: `token ${token}`, Accept: "application/vnd.github+json", "Content-Type": "application/json" },
                body: deleteBody,
            });

            if (deleteRes.ok) {
                console.log(`‚úÖ [Proxy-Backup] Gel√∂scht: ${file.name}`);
            } else {
                const errorData = await deleteRes.json();
                console.error(`‚ùå [Proxy-Backup] L√∂schfehler f√ºr ${file.name}: ${deleteRes.status} ${JSON.stringify(errorData)}`);
            }
        }
    } catch (e) {
        console.error("‚ùå [Proxy-Backup-Fehler] Fehler bei der Bereinigung alter Backups:", e.message);
    }
}


;(async () => {
  console.log("üß© [Proxy-Backup] Initialisiere automatisches Backup-System ...");

  try {
    if (!PROXY_APP_ID || !PROXY_INSTALLATION_ID || !PROXY_KEY || !PROXY_REPO_OWNER || !PROXY_REPO_NAME) {
      console.error("‚ùå [Proxy-Backup] Fehlende Proxy-Variablen. Backup abgebrochen.");
      return;
    }

    // 1. Lokale Datei lesen und Hash berechnen
    const currentFilePath = path.join(process.cwd(), "server.cjs"); 
    const serverData = fs.readFileSync(currentFilePath, "utf-8");
    const currentFileHash = crypto.createHash('sha256').update(serverData, 'utf8').digest('hex');

    // 2. Token abrufen und Remote-Hash pr√ºfen
    const token = await getBackupInstallationToken(); 
    const { hash: remoteHash, sha: remoteHashSha } = await getLatestRemoteHash(token);

    // 3. Bedingung: Wenn Hash gleich, Backup √ºberspringen
    if (currentFileHash === remoteHash) {
        console.log(`‚úÖ [Proxy-Backup] Inhalt hat sich NICHT ge√§ndert (${currentFileHash.substring(0, 10)}...).
Backup √ºbersprungen.`);
        return; 
    }

    console.log("üîÑ [Proxy-Backup] √Ñnderung im Dateiinhalt festgestellt. Backup wird erstellt...");
// üîë NEUE LOGIK F√úR LOKALISIERTEN ZEITSTEMPEL (CET/CEST)
    const cetDate = new Date().toLocaleString("sv-SE", { 
        year: 'numeric', month: '2-digit', day: '2-digit',
        hour: '2-digit', minute: '2-digit', second: '2-digit',
        hourCycle: 'h23', 
        timeZone: 'Europe/Berlin'
    });
const timestamp = cetDate.replace(' ', '_').replace(/:/g, '-').replace(',', '');


    // 4. Lokales Backup
    const backupDir = path.join(process.cwd(), "backups");
if (!fs.existsSync(backupDir)) { fs.mkdirSync(backupDir); }
    fs.writeFileSync(path.join(backupDir, `server_backup_${timestamp}.cjs`), serverData);
    console.log("üíæ [Proxy-Backup] Lokale Sicherung erstellt.");
// 5. Remote Backup (Erstellen der neuen zeitgestempelten Backup-Datei)
    const remoteBackupPath = `backups/server_backup_${timestamp}.cjs`;
const contentEncoded = Buffer.from(serverData, "utf-8").toString("base64");
    
    const backupUrl = `https://api.github.com/repos/${PROXY_REPO_OWNER}/${PROXY_REPO_NAME}/contents/${remoteBackupPath}`;
    const backupBody = JSON.stringify({
      message: `üîÑ Auto-Backup ${timestamp} (Hash: ${currentFileHash.substring(0, 10)})`,
      content: contentEncoded,
      branch: PROXY_BRANCH || "main",
    });
const uploadRes = await fetch(backupUrl, {
      method: "PUT",
      headers: { Authorization: `token ${token}`, Accept: "application/vnd.github+json", "Content-Type": "application/json" },
      body: backupBody,
    });
if (!uploadRes.ok) {
        const errorData = await uploadRes.json();
throw new Error(`Backup Upload Error: ${uploadRes.status} ${JSON.stringify(errorData)}`); 
    }

    console.log("‚úÖ [Proxy-Backup] Backup erfolgreich ins Proxy-Repo hochgeladen.");
// 6. Hash-Datei aktualisieren (WICHTIG f√ºr den n√§chsten Lauf)
    const hashPath = 'backups/current_server_cjs_hash.txt';
    const hashUpdateUrl = `https://api.github.com/repos/${PROXY_REPO_OWNER}/${PROXY_REPO_NAME}/contents/${hashPath}`;
// Body-Objekt erstellen
    const hashUpdateObject = {
        message: `ü§ñ Update server.cjs hash to ${currentFileHash.substring(0, 10)}`,
        content: Buffer.from(currentFileHash, "utf-8").toString("base64"),
        branch: PROXY_BRANCH ||
"main",
    };
    
    // FIX f√ºr 404: Der SHA wird NUR hinzugef√ºgt, wenn wir ihn von GitHub beim Abruf erhalten haben.
if (remoteHashSha) {
        hashUpdateObject.sha = remoteHashSha;
}
    const hashUpdateBody = JSON.stringify(hashUpdateObject);


    const hashUpdateRes = await fetch(hashUpdateUrl, {
        method: "PUT", 
        headers: { Authorization: `token ${token}`, Accept: "application/vnd.github+json", "Content-Type": "application/json" },
        body: hashUpdateBody,
    });
if (!hashUpdateRes.ok) {
        const errorData = await hashUpdateRes.json();
        console.error("‚ö†Ô∏è [Proxy-Backup] Hash-Update fehlgeschlagen:", errorData);
throw new Error(`Hash Update Error: ${hashUpdateRes.status} ${JSON.stringify(errorData)}`); 
    }
    console.log("‚úÖ [Proxy-Backup] Hash-Datei erfolgreich aktualisiert. N√§chster Lauf wird √ºbersprungen, falls keine √Ñnderung.");

    // 7. NEUE LOGIK: Bereinigung alter Backups (Maximal 3 behalten)
    await cleanupOldBackups(token, 3);

} catch (error) {
    console.error("‚ùå [Proxy-Backup-Fehler]", error);
  }
})();
